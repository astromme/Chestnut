@article{gpu-optimizations,
abstract = {Recent years have seen a trend in using graphic pro- cessing units (GPU) as accelerators for general-purpose comput- ing. The inexpensive, single-chip, massively parallel architecture of GPU has evidentially brought factors of speedup to many nu- merical applications. However, the development of a high-quality GPU application is challenging, due to the large optimization space and complex unpredictable effects of optimizations on GPU program performance. Recently, several studies have attempted to use empiri- cal search to help the optimization. Although those studies have shown promising results, one important factorprogram inputsin the optimization has remained unexplored. In this work, we initiate the exploration in this new dimension. By conducting a series of measurement, we find that the ability to adapt to program inputs is important for some applications to achieve their best performance on GPU. In light of the findings, we develop an input-adaptive optimization framework, namely G-ADAPT, to address the influence by constructing cross-input predictive models for automatically predicting the (near-)optimal configurations for an arbitrary input to a GPU program. The results demonstrate the promise of the framework in serving as a tool to alleviate the productivity bottleneck in GPU programming.},
author = {Liu, Y and Zhang, E Z},
doi = {10.1109/IPDPS.2009.5160988},
isbn = {9781424437511},
journal = {2009 IEEE International Symposium on Parallel Distributed Processing},
pages = {1--10},
publisher = {IEEE},
title = {{A cross-input adaptive framework for GPU program optimizations}},
url = {http://doi.ieeecomputersociety.org/10.1109/IPDPS.2009.5160988},
year = {2009}
}

@inproceedings{openmp-to-cuda,
abstract = {GPGPUs have recently emerged as powerful vehicles for general-purpose high-performance computing. Although a new Compute Unified Device Architecture (CUDA) programming model from NVIDIA offers improved programmability for general computing, programming GPGPUs is still complex and error-prone. This paper presents a compiler framework for automatic source-to-source translation of standard OpenMP applications into CUDA-based GPGPU applications. The goal of this translation is to further improve programmability and make existing OpenMP applications amenable to execution on GPGPUs. In this paper, we have identified several key transformation techniques, which enable efficient GPU global memory access, to achieve high performance. Experimental results from two important kernels (JACOBI and SPMUL) and two NAS OpenMP Parallel Benchmarks (EP and CG) show that the described translator and compile-time optimizations work well on both regular and irregular applications, leading to performance improvements of up to 50X over the unoptimized translation (up to 328X over serial).},
author = {Lee, Seyong and Min, Seung-Jai and Eigenmann, Rudolf},
booktitle = {Proceedings of the 14th ACM SIGPLAN symposium on Principles and practice of parallel programming},
doi = {10.1145/1504176.1504194},
isbn = {9781605583976},
issn = {03621340},
keywords = {automatic translation,com,cuda,gpu,openmp},
number = {4},
organization = {ACM New York, NY, USA},
pages = {101--110},
publisher = {ACM},
series = {SIGPLAN Not. (USA)},
title = {{OpenMP to GPGPU: a compiler framework for automatic translation and optimization}},
url = {http://portal.acm.org/citation.cfm?id=1504176.1504194},
volume = {44},
year = {2009}
}

@inproceedings{mint-paper,
  author = {Unat, Didem and Cai, X and Baden, Scott},
  booktitle = {Proceedings of the ACM International Conference on Supercomputing ICS},
  title = {{Mint: Realizing CUDA performance in 3D stencil methods with annotated C}},
  year = {2011}
  annote = "
Mint targets the stencil method for memory access, where each element can be computed from a stencil of nearby elements. This allows for efficient parallelization and memory access. Built from openmp style compiler pragmas, the Mint architecture transparently translates normal C source code into cuda code. This paper shows some simple examples of gradient, laplacian and other common functions which can be computed using their framework. Example applications include heat maps and fluid simulations. 2D and 3D support is given.\\
\\
Mint shows some promising results where their work is 3 to 6 times faster than the comparison OpenMP code. 3 levels of optimizations are given, including shared memory, loop aggregation and register optimizations.
}


@article{hicuda-paper,
  author = {Tianyi David Han and Tarek S. Abdelrahman},
  title = {{hiCUDA: A High-level Directive-based Language for GPU Programming}},
  journal = {Proceedings of 2nd Workshop on General Purpose Processing on Graphics Processing Units},
  volume = {383},
  pages = {52--61},
  month = {March},
  year = {2009},
  annote = "
Converting existing programs from a singly threaded model to the CUDA model can take significant time and experience, partly because of the mechanical conversions needed such as computing array references instead of using loops. hiCUDA is an example of an OpenMP style C preprocessor language that allows the programmer to wrap his singly threaded loops to allow them to be automatically parallelized by CUDA. It was found that examples written with the prototype hiCUDA implementation did not perform significantly worse than the same examples written directly with CUDA. Additionally, the hiCUDA examples were more concise and better fit into existing loop-based computation models that are often present in single threaded programs. \\
\\
This research is useful to us because it shows a similar abstraction to what we want to accomplish. We think that ease-of-use enhancements can emerge from hiding certain aspects of writing software for the GPU, such as moving data between the system RAM and the GPU RAM, writing functions to figure out what parts of the data to modify and writing code to physically convert data from its native format to arrays and back again. This is also a good example of showing how pre-existing parallel concepts and frameworks (shared-memory loop-parallelism OpenMP) can be extended to the graphics card.
"
}

@article{scratch-paper,
   author = {Mitchel Resnick and John Maloney and Andr\'{e} Monroy-Hern\'{a}ndez and Natalie Rusk and Evelyn Eastmond and Karen Brennan and Amon Millner and Eric Rosenbaum and Jay Silver and Brian Silverman and Yasmin Kafai},
   title = {{Scratch: Programming for All}},
   journal = {Communications of the ACM},
   volume = {52},
   number = {11},
   pages = {60--67},
   month = {November},
   year = {2009},
   annote = "
     Scratch is a programming language developed at MIT that provides an appealing graphical environment aimed at introducing programming to 8-16 year olds. Familiar programming constructs like conditionals and loops are present, but are presented in an intuitive manner. The authors hold that the ``computational thinking'' that programming affords is a valuable tool that should be encouraged but is currently lacking in non-Computer Science curriculum. Most children's digital fluency is defined by ``reading'' but not ``writing.'' The article discusses some of the design decisions that went into the language. To make the language appeal to their target, the authors wanted Scratch to be more tinkerable, meaningful, and social. Additionally, we can think of any programming language as having a ``low floor'' (easy to begin), ``high ceiling'' (ability to become more complex), and ``wide walls'' (support for a diverse range of projects). Scratch appropriately focuses on the low floor and wide walls, arguing that a higher ceiling is outside the scope of a learning language, and if students are interesting in ``raising the ceiling'' they can and should look to more established languages like C or Python. \\ \\

     This article raises some important questions that we need to consider as we design our own language. What is the level of the floor and ceiling? How wide are the walls to be? Scratch's design is very heavily influenced by a target audience. We need to consider our target audience carefully and make sure that our language is appealing. Our project will not be an introductory language to programming as a whole, but will rather be targeted at people with some programming experience who have never ventured into the world of general purpose graphics processing.
     "
}

@article{2nd-language-learning,
  author = {Stephanie A. Robertson and Martin P. Lee},
  title = {{The Application of Second Natural Language Acquisition Pedagogy to the Teaching of Programming Languages -- A Research Agenda}},
  journal = {SIGCSE Bulletin},
  volume = {27},
  number = {4},
  pages = {9--12},
  month = {December},
  year = {1995},
  annote = "
    The authors of this paper argue that teaching a second natural language and teaching a first programming language are not as different as one might suspect. While much research has gone into how best to communicate the syntax and semantics contained in a natural language, teaching programming languages is sorely in need of such instruction. Central to that argument is a concentration on writing code over reading it. A large and important part of learning a natural language is an ability to read, digest, and internalize the information being presented. So, the authors write, we must teach our students to read the code as well as write it. \\ \\

    Though we are in the business of {\em writing} a language, we need to place high value on the {\em readability} of our language. This is, after all, intended to be a teaching language and we want it to be accessible to our audience. By reinforcing the message that reading existing code is important we can help train our users to really get a grasp of the concepts at work.
    "
}

@article{cuda-algorithms-teaching,
  author = {Quoc-Nam Tran},
  title = {{Teaching Design \& Analysis of Multi-Core Parallel Algorithms Using CUDA}},
  journal = {Journal of Computing Sciences in Colleges},
  volume = {25},
  number = {4},
  pages = {7--14},
  month = {April},
  year = {2010},
  annote = "
    Most of an undergraduate's computer science training is involved with writing single-threaded, sequential code. Since single-core CPUs are reaching limits on a hardware level, a shift is underway towards multi-core machines. This paper describes how to harness a GPU for general purpose computing using CUDA and how one might implement this into a college curriculum. A high-level overview of the CUDA model is presented along with a sample algorithm that illustrates the usefulness of teaching this new paradigm. \\ \\

    The most useful part of this paper was the overview of the process that goes into writing a CUDA program. Given a bird's-eye view of the CUDA model, we can begin to get an idea of how to shape our own language. For example, in CUDA the programmer must decide how many threads will be executed. We would like to hide this detail from the user since it is too low level. Given this kind of big-picture writeup, we can begin to see what may cause problems and what patterns may emerge.
    "
}

@MISC{thrust,
  author = "Jared Hoberock and Nathan Bell",
  title = "Thrust: A Parallel Template Library",
  year = "2011",
  url = "http://code.google.com/p/thrust/",
  note = "Version 1.4"
}
