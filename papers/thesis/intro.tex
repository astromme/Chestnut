\section{Introduction}

Students are often introduced to parallel programming with the concept of heavyweight processes or threads. With this introduction students are encouraged to think in terms of how threads communicate with each other. This works well for concepts such as the producer-consumer problem and for parent-child models but a more simple parallel model is sufficient for a wide array of problem domains.

We observe that often the same operation is performed to each element in a large array of data. This can be seen from simple calculations (add 4 to each element of an array) to more complex situations such as generating an updated array value by some combination of the neighboring points (this is used in simulations such as heat flow models). In traditional sequential implementations of these computations there is a core operation that is performed many times within one or more loops. When updating element of data is independent from updating any other element the whole computation can be parallelized with a simple parallel model. 

The Chestnut parallel model assumes that a program exists as sequential code with small self-contained blocks of parallel code interspersed throughout. Parallel blocks, or contexts, exist to update arrays with new values in parallel. When declarating a parallel context the programmer specifices a foreach loop based on some array; one iteration of the loop is run for each element in the array. Each iteration of this loop is performed in a separate thread in parallel with all other iterations. Each thread has access to the state of all arrays as they were before the loop started. This means that even if one thread modifies a value in an array within a parallel context all other threads within this context can safely use the same location in the array; they will get the old values instead. There is an implicit synchronization barrier at the end of each parallel context where all threads must finish their computations before the sequential code following the parallel contex can resume. After the parallel context the values of any arrays that have been modified now reflect the new values rather than the pre-parallel-context values. Subsequent array accesses get the new values.

This parallel model fits the graphics card well--typically parallel contexts will perform the same operation across all threads, a condition which is needed for the processors in a graphics card to be fully utilized.

A difficult part with any parallel computation is the specification of the parallelism. In general Chestnut programs will specify their parallelism based on the output arrays--one computation is one for each element in the output array, combining the necessary inputs. For an example matrix multiplication application the foreach loop would iterate over the output array and each thread would access the necessary row and column of the input arrays to perform the computations necessary to compute that output value. 

Talk about chestnut being research into a simplification of the GPU general purpose programming model. Locality is important, simple programs are simple. Simulations, visualizations. Visualizations stay on the GPU, and pixel rasterization uses the same language.
