\documentclass{article}
\usepackage{fullpage}
\usepackage{apacite}
\newcommand{\comment}[1]{}

\begin{document}
\title{CS87 Proposal}
\author{Andrew Stromme \& Ryan Carlson}
\date{March 26, 2010}
\maketitle

\section{Introduction}

The General Purpose Graphics Processing Unit (GPGPU) model of programming allows a programmer to use the power of a Graphics Processing Unit (GPU) to help speed up general computation tasks. GPUs are designed to process large amounts of data very quickly and efficiently, but they have a limited instruction set. Where Central Processing Units (CPUs) are the ``jack of all trades'' of computation, GPUs are the specialists. Sophisticated tools like Compute Unified Device Architecture (CUDA) created by nVidia and the open-source OpenCL are available to facilitate general computation on these devices. This allows for massive speedup compared to algorithms implemented on single-threaded CPUs. Unfortunately, programming for a GPU is very different from programming for a CPU for a number of reasons. First, the GPU is optimized for graphics processing and thus has a very different structure. Second, the designers want to tightly couple their languages with existing programming languages to facilitate transition to GPGPU programs. Obviously, these already-existing languages were not designed with GPUs in mind and so some of the transitions are forced. Finally, a primary focus of GPGPU language design is to keep it at a low level. The designers want their users to be able to tweak their performance along every possible axis. All of GPGPU languages like CUDA and OpenCL are barriers to the casual programmer.

Our goal is to create a simple language that will expose the core concepts of the GPGPU programming paradigm. Since programming on a GPU is different, we feel it is necessary to work with a language that is different. This language will be specifically tuned to shed light on the advantages of using a GPU. Once the programmer has written code in our language, it will be translated into CUDA code and then compiled. Thus our language is an abstraction of and a mapping onto CUDA. Additionally, a high value is being placed on making the language intuitive and above all readable. As a simple heuristic, code should be immediately comprehensible to even a casual programming audience. Less emphasis will be placed on efficiency, though we still anticipate speedups over CPU runtimes. If we can get a casual programmer interested in GPGPU programming, we hope our language gives them the tools to dig deeper and experiment with CUDA or OpenCL.

\section{Related Works}

We can break up the designing of this project into two broad categories: pedagogy of programming languages and the paradigm presented by CUDA. In other words, we need to equip ourselves with the tools to design {\em a} language, and then we need to figure out what is necessary to design {\em the} language we are aiming for. Since our language aims to be a teaching language, we look towards {\em Scratch}, a graphical programming language developed at the Massachusetts Institute of Technology (MIT), for guidance. Scratch provides an appealing graphical environment aimed at introducing programming to 8-16 year olds. Familiar programming constructs like conditionals and loops are present, but are presented in an intuitive manner. The authors hold that the ``computational thinking'' that programming affords is a valuable tool that should be encouraged but is currently lacking. Most children's digital fluency is defined by ``reading'' but not ``writing.'' The article discusses some of the design decisions that went into the language. To make the language appeal to their target, the authors wanted Scratch to be more tinkerable, meaningful, and social. Additionally, we can think of any language as having a ``low floor'' (easy to begin), ``high ceiling'' (ability to become more complex), and ``wide walls'' (support for a diverse range of projects). Scratch appropriately focuses on the low floor and wide walls, arguing that a higher ceiling is outside the scope of a learning language, and if students are interesting in ``raising the ceiling'' they can and should look to more established languages like C or Python \cite{resnick}.

Another interesting paper draws parallels between teaching a second natural language and teaching a first programming language. The authors claim that these processes are not as different as one might expect. While much research has gone into how best to communicate the syntax and semantics contained in a natural language, teaching programming languages is sorely in need of such instruction. Central to that argument is a concentration on writing code over reading it. A large and important part of learning a natural language is an ability to read, digest, and internalize the information being presented. So, the authors write, we must teach our students to read the code as well as write it. This informs our disposition towards creating a language that enforces code readability \cite{robertson}.

We can now safely turn to work regarding CUDA. Most of an undergraduate's computer science training is involved with writing single-threaded, sequential code. Since single-core CPUs are reaching limits on a hardware level, a shift is underway towards multi-core machines. This paper describes how to harness a GPU for general purpose computing using CUDA and how one might implement this into a college curriculum. A high-level overview of the CUDA model is presented along with a sample algorithm that illustrates the usefulness of teaching this new paradigm. The most useful part of this paper was the overview of the process that goes into writing a CUDA program. Given a bird's-eye view of the CUDA model, we can begin to get an idea of how to shape our own language. For example, in CUDA the programmer must decide how many threads will be executed. We would like to hide this detail from the user since it is too low level. Given this kind of big-picture writeup, we can begin to see what may cause problems and what patterns may emerge \cite{tran}.

Finally, we need to consider the paradigm of actually programming for a graphics processor. Converting existing programs from a singly threaded model to the CUDA model can take significant time and experience, partly because of the mechanical conversions needed such as computing array references instead of using loops. hiCUDA is an example of an OpenMP style C preprocessor language that allows the programmer to wrap his singly threaded loops to allow them to be automatically parallelized by CUDA. It was found that examples written with the prototype hiCUDA implementation did not perform significantly worse than the same examples written directly with CUDA. Additionally, the hiCUDA examples were more concise and better fit into existing loop-based computation models that are often present in single threaded programs. However, the language is still based around C using compiler directives and as such may not reflect or emphasize the models that one must internalize to understand GPGPU programming. Still, hiCUDA shows a similar abstraction-minded attitude and gives us an idea of what can be accomplished when building on top of CUDA \cite{han}.

\section{Solution}

\section{Experiments}

\section{Equipment Needed}

\section{Schedule}

\section{Conclusion}

GPUs are very powerful devices. But to use them for general purpose computation we need to jump through hoops by using CUDA. Additionally, CUDA is prohibitively complicated for many programmers interested in speeding up their computation but not necessarily learning the ins and outs of a GPU. To ease the transition from CPU programming to GPU programming we aim to create a new language that simplifies the process. We expect to identify high level patterns and express them in a way that exposes the paradigms that a programmer would need to effectively use the GPU. While we do anticipate slowdowns from handwritten CUDA code, we hope to create and reinforce good GPGPU coding habits that will allow our users to move onto more extensive projects in the future.


\bibliographystyle{apacite}
\bibliography{references}

\end{document}
