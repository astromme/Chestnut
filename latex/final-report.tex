\documentclass{article}
\usepackage{fullpage}
\usepackage{apacite}
\usepackage{url}
\usepackage[bottom]{footmisc} % make sure footnote is at bottom of page
\newcommand{\comment}[1]{}

\usepackage{float}
\newfloat{Algorithm}{h!}{}{}

% program-related commands
\usepackage{program}
\renewcommand{\|}{\origbar} % use this instead of '|' because program package redefines '|'
\renewcommand{\WHILE}{\mbox{{\bf while} }\tab}
\renewcommand{\FOR}{\mbox{{\bf for} }\tab}
\renewcommand{\IF}{\mbox{{\bf if} }\tab}
\newcommand{\IN}{\mbox{ {\bf in} }}

\newcommand{\code}[1]{\texttt{#1}}

\begin{document}
\title{Chestnut: Simplifying General Purpose Graphics Processing}
\author{Andrew Stromme \& Ryan Carlson}
\date{May 10, 2010}
\maketitle

\begin{abstract}
\end{abstract}

\section{Introduction (RC)}

Say why GPGPU is cool but difficult. We have provided an easier way to deal with it. With us, you get the performance gains without the steep learning curve.

\section{Related Works (RC)}

Basically just plop our annotated bibliography in here.

\section{General Purpose Graphics Processing}

\subsection{Structure of Graphics Processing Unit}

A graphics processing unit is a dedicated piece of hardware separate from the CPU. It was originally designed for graphics processing where each pixel is run through a series of transformations and filters before it is shown on the screen. However, the massively parallel architecture of the GPU is now being used for more general purpose programming. 

Modern GPUs are structured with dozens of individual processing units organized into blocks on the card. Each of these blocks has access to a small amount of fast block-shared memory (similar in speed to the cache on a CPU) and the vastly slower but much larger GPU main memory. Unlike CPUs, GPUs can not assign and run different threads of computation simultainiously on their many processing units. Instead, they run a single process (called a kernel) on every unit. These kernels are able to access both types of memory and perform their computations in parallel. The programmer is responsible for writing this kernel, as well as for designing the way in how threads will be assigned data. This can be a complex task that requires some low level knowledge on how the GPU is designed as well as knowledge of special extensions to a programming language (e.g. CUDA or OpenCL) that can tell the GPU what to do.

\subsection{Paradigm/Data Flow}

GPUs lend themselves to certain types of programming, and these are not always the same as what traditional parallel models offer. For chestnut we have focused on a single paradigm that allows for significant and semi-automatic parallelization. This model is data oriented and focuses on the interactions between functions and data. Large chunks of data are assumed, such as large 2d arrays where each element in the array is a scalar value. Functions are written to perform the same operation on each element of data with no ordering constraints. This means that the operation can be applied to each element in any order, and more importantly means that the operatioins can be applied in parallel. This model of programming maps quite nicely onto the core parts of GPU programming. A single program can be written as a kernel and each running instance of that kernel can operate on a small chunk of the overall data.

\subsection{Tools}

There are a number of tools available to interface with the GPU. Two we have chosen are CUDA and Thrust.

\subsubsection{CUDA}

Developed by Nvidia. Allows fine-tooth control over GPU. Very verbose and C-like

\subsubsection{Thrust (RC)}

Much more C++-y. Easier to grasp. More like programming for CPU but still need to know some stuff about GPU

\subsubsection{Thrust Functions (RC)}

We might want an overview of thrust functions here. Might also want that in Translating Chestnut to Thrust

\section{Chestnut}

\subsection{Overview}

Chestnut is a graphical environment for parallel programming. It is composed of a graphical frontend, an intermediate language and a compiler that translates the intermediate language into thrust C++ code. The main point of entry for users is imagined to be the Chestnut GUI, with the simple code underlying this to be a small but more complicated step towards actual thrust code. Chestnut is based on the data-oriented programming model that was explained earlier. This is enforced through the gui and through the limited syntax offered by the underlying program.

\subsection{Target Audience}

Chestnut is meant to be a relatively simple introduction to parallel GPU programming. We expect little to no experience with C++ and see two categories of users with this type of knowledge. The first group is computer science/programming students who have not had any exposure or introduction to GPU programming before. The second group is non computer science programmers who could benefit from parallelizing their computations. Examples of the latter might be other natural science researchers who have written number-crunching programs in scripting languages who have parallelizable algorithms.

With both of these cases the learning curve can be too steep to encourage these programmers to learn about GPU programming. With the underlying chestnut code we are interested in having a similar syntax to python or javascript, where the code is similar to C but without some of the extra syntax such as namespaces, headers vs source files, includes and other complexities. We hope that a person with some knowledge of programming and with a problem could use Chestnut to write a basic solution to this problem.

\subsection{Target Applications}

At this point Chestnut is targeted towards embarrassingly parallel problems. We reason that there are a vast number of embarrasingly parallel problems out there that have not been written to take advantage of the GPUs because of the learning curve associated with GPU programming. Those are our target problems. Additionally, because the target audience is not advanced computer science programmers we are not convinced that the extra complexity that would be needed by offereing things like more fine-grained synchronization, lower level kernel specification and control over how blocks of threads are distributed is warrented.

\subsection{Goals}

With both the target audience and the target applications in mind we have identified a few important goals for Chestnut. Primarily it should expose the data-focused model that can be translated to the GPU easily. This can be done by imposing limitations on the gui and on the core language syntax. Secondly we want Chestnut to be modular. This means separating the gui interface from the underlying lanugage, and providing a compiler for this underlying language that could be changed in the future to not depend on Thrust or CUDA should either become eclipsed by other GPU programming environments. Chestnut needs to be discoverable. The gui frontend and its visual and drag and drop interface is a direct result from this, where the user can see how things are supposed to be connected. Lastly we are still interested in achieving a speedup when compared to the CPU. Although Chestnut is not about the fastest possible runtime on the GPU compared to other GPU programs we still want it to be faster than running the same computations on the CPU because even with chestnut there is still a learning curve to working on the GPU.

\section{Chestnut Frontend (AS)}

\subsection{Themes (?)}

Data-centric programming model. Drag-and-Drop interface. Forces GPU programming model.

\subsection{Primitives}

DataBlocks, Values, and Functions.

\subsection{Translate GUI to Chestnut Code (RC)}

Visit each node. Each node knows its contribution. Variable Declarations and Function calls.

\section{Chestnut Backend (RC)}

Once the GUI to Chestnut translation is finished, we have Chestnut code output. In this section we discuss the process that brings our Chestnut code to executable object code.

\subsection{Process}

%Lex tokenizes, Yacc accepts sequences of tokens. From Yacc we call helper functions which write Thrust code to disk. Compile that code using nvcc
The translation from Chestnut to executable is a four step process. First, we use Flex\footnote{An open-source version of Lex, found at \url{http://flex.sourceforge.net/}.} to tokenize the Chestnut code. For each valid word in the language, a token is created. For example, if Flex reads the word \code{map} it outputs the \code{TOKMAP} token. While Flex can return just a token without any extra context, it also has the ability to recognize and then save the input in a string or integer and pass that information along as well. In this case, one might have the variable \code{var1} which gets mapped to the {\em identifier} token \code{ID} and additionally saves the string \code{"var1"} to pass along to the next stage. Every valid word or character in the file (including braces, parentheses, etc.) must be tokenized to be accepted and passed along. If a token is not found, Flex will exit with a syntax error. This provides a simple but effective first level of error checking for our code.

As Flex tokenizes the file, it passes each token along to Bison\footnote{The GNU version of Yacc, found at \url{http://gnu.org/software/bison/}.} for processing. Bison generates an LALR(1) (one token Look-Ahead LR) or a GLR (generalized LR) parser. Using very simple regular expressions, Bison generates code that analyzes each token and matches it to predefined acceptable sequences of tokens. For example, suppose the input code for a reduction was \code{"result = reduce(data)"}\footnote{See section \ref{sec:specifications} for a full description of syntax}. The resulting sequence of acceptable tokens to check for would be \code{"ID ASSIGN TOKREDUCE LPAREN ID RPAREN"}. Once a valid sequence of tokens is identified, we gather the appropriate information (variable names, operators, parameters) and pass them into a utility function (part of a utility class) that takes care of writing all the necessary Thrust code to disk. Once Thrust code has been written it can be compiled to object code using the CUDA compiler, \code{nvcc}.

The utility class knows what files we eventually want to write to, keeps a symbol table, and has all the necessary functions to write appropriate Thrust code. We have broken each file into four general regions: headers (\code{\#includes}), function declarations, the \code{main} function, and function definitions. Each region is represented as an STL vector of strings. Thus, any header files we want to include in a given file or code we want to write to \code{main} get stored as entries in their respective vectors until the Chestnut code is finished being parsed. Once our parser reaches the end of the file, we execute a single write to disk.

The symbol table is implemented as an STL vector of \code{symbol\_entry} structs. A \code{symbol\_entry} is a simple container for four fields. The first is the name of the function or variable. The second is that object's type, either \code{int} or \code{float} at present (see section \ref{sec:specifications} for a complete discussion of supported types). The third field is the category of the entry. It can be either a \code{FUNCTION} or two different variable types. If the variable is a single value, we assign the category \code{VARIABLE\_SCALAR}. If the variable is a vector (or DataBlock, in the frontend terminology) we assign it \code{VARIABLE\_VECTOR}. Finally, each entry has a scope. This field is not currently being used and is by default set to zero. 

\subsection{Specifications}
\label{sec:specifications}

%Support ints, floats. Functions: map, reduce, sort, print, read, write. Ask Tia if we should put specifications here or in an appendix or what.
We currently support two basic variable types and six default functions. Here we discuss them and specify each of their uses. This section will introduce the reader to basic Chestnut syntax and give the reader a sense of the level of abstraction used.

At this early stage, the only data types we support are standard C++ \code{int} and \code{float}. Considering our target audience and what we anticipate them using Chestnut for, we believe that in an overwhelming number of cases these basic number types will be sufficient. Users will be acting primarily on large numerical datasets and our language absolutely supports that. In the future, we would like to implement more default types and allow users to define their own types, described in section \ref{sec:future}. {\bf talk about variable declarations?}

Chestnut currently has six functions that operate on data: \code{map}, \code{reduce}, and \code{sort} all manipulate the data in some way; \code{read}, \code{write}, and \code{print} comprise the Input/Output interface of our language. The general syntax for a function call in Chestnut is
\begin{center}
  \code{result = functionName($param_1,\;param_2, \;\ldots, \;param_k,$ inputData)}
\end{center}
The result of the operation is always stored in a variable (either a scalar or vector) to be used later. Every function also operates on some block of data. These two specifications enforces the data-centric model of programming. Note that it is easy to modify data in place by setting both the result and input to the same variable. Each $param_i$ is an extra parameter that specifies some option of the function. There can be zero or more such parameters. 

Let's consider the \code{map} function, the most complex of the default functions. A \code{map} takes a block of data and applies the same modification to every element. For example, we could add 2 to every element in an array. Our \code{map} also works as a convolution operator, applying an operator to corresponding elements in two arrays. Formally, given two 2-dimensional arrays \code{A,B} convolved into a resulting array \code{C}, each with indices \code{i,j}, the resulting array would have the property \code{C[i][j]~=~A[i][j]~+~B[i][j]} for all \code{i,j}.


\subsection{Sample Code (?)}

Do we want to show some samples? Use verbatim!

\subsection{Translating Chestnut Code to Thrust}

Talk about the assumptions we need to make (when/where memory is allocated), some optimizations (\code{constant\_iterators}), the basic Thrust functions used.

\section{Experiments (RC)}

Run both qualitative experiments -- compare code samples -- and quantitative experiments with runtimes. We show that, subjectively, our code is easier to read, if less powerful. BUT!\ runtimes are very similar.

\subsection{Qualitative}

Compare Chestnut to Thrust to CUDA

\subsection{Quantitative}

Runtimes.

\section{Future Work}
\label{sec:future}

\begin{itemize}
  \item Customizable Functions and DataTypes
  \item More default Functions and DataTypes
  \item Refine GUI
  \item For-loops
\end{itemize}

\end{document}

