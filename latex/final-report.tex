%\documentclass{article}
\documentclass[twocolumn]{article}
\usepackage{fullpage}
\usepackage{multirow}
\usepackage{apacite}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{url}
\usepackage{fixltx2e} % so that 'starred' figures aren't out of order with 'non-starred' figures
\usepackage[bottom]{footmisc} % make sure footnote is at bottom of page
%\newcommand{\comment}[1]{}

\usepackage{float}
\newfloat{Algorithm}{h!}{}{}
\floatstyle{ruled}
\newfloat{Code Snippet}{h!}{}{}

% program-related commands
\usepackage{program}
\renewcommand{\|}{\origbar} % use this instead of '|' because program package redefines '|'
\renewcommand{\WHILE}{\mbox{{\bf while} }\tab}
\renewcommand{\FOR}{\mbox{{\bf for} }\tab}
\renewcommand{\IF}{\mbox{{\bf if} }\tab}
\newcommand{\IN}{\mbox{ {\bf in} }}

\newcommand{\code}[1]{\texttt{#1}}

\begin{document}
\title{Chestnut: Simplifying General Purpose Graphics Processing}
\author{Andrew Stromme \& Ryan Carlson}
\date{May 10, 2010}
\maketitle

\begin{abstract}
  %Hello there fellow travelers. Have I got a story for you!
  Graphics processing units (GPUs) are powerful devices capable of rapid parallel computation, but using them for general computing needs can be a difficult and tedious task. We have created a graphical language called Chestnut that simplifies the process of programming for a GPU. The graphical environment is supported by a backend language that can be translated into GPU-ready code. Our language is intuitive, discoverable, and supports common operations used by GPU programmers. Runtime tests demonstrate that our code is much faster than sequential code run on the central processing unit (CPU) and is often comparable to optimized code written specifically for the GPU.
\end{abstract}

\section{Introduction}

%Say why GPGPU is cool but difficult. We have provided an easier way to deal with it. With us, you get the performance gains without the steep learning curve.

There are a number of advantages of using a GPU for general purpose computation. Since GPUs were designed to process large amounts of data in parallel very quickly, a user can greatly speed up his or her runtimes by translating sequential code into code run on GPU. Certain problems lend themselves to such a conversion, which we call embarrassingly parallel problems. In this class of programs, one generally wants to perform the same operation on every element of large chunks of the data. For example, a user might want to double every element in an array, find the sum of every element, or sort the values. Unfortunately, this translation process can be tedious and prohibitively involved. The tools used to program on the GPU require experience with C or C++, a solid grounding in memory management, and an understanding of the interactions between the CPU and GPU. 

Chestnut is our solution to these problems. It provides a graphical environment in which users can drag and drop blocks of data along with functions that operate on them. Clearly marked inputs and outputs allow the user to connect data to functions. Thus at the graphical level we enforce the paradigm required to program on general purpose on graphics processing units (GPGPU) which pushes large blocks of data through functions that operate on each element and outputs the result as more blocks of data. The graphical frontend is then translated into a text-based backend language which also reinforces the data-centric model.

The rest of the paper is organized as follows. Section \ref{sec:relatedWorks} discusses work related to both introductory programming languages and GPGPU languages. In section \ref{sec:GPGP} we detail the Graphics Processing Unit and describe the tools used to program on it. We give a broad overview of Chestnut in section \ref{sec:chestnut} before delving into the graphical frontend (section \ref{sec:chestnutFrontend}) and the text-based backend (section \ref{sec:chestnutBackend}). Qualitative and quantitative experiments are presented in section \ref{sec:experiments} showing our code is more readable than the code constructed using current tools and provides comparable speedup benefits. Finally, we add some concluding remarks in section \ref{sec:conclusion} and discuss the future of the project in section \ref{sec:future}.

\section{Related Works}
\label{sec:relatedWorks}

%Basically just plop our annotated bibliography in here.

We can break up designing this project into two broad categories: pedagogy of programming languages and the paradigm presented by CUDA. In other words, we need to equip ourselves with the tools to design {\em a} language, and then we need to figure out what is necessary to design {\em the} language. Since our language aims to be a teaching language, we look towards {\em Scratch}, a graphical programming language developed at the Massachusetts Institute of Technology (MIT), for guidance. Scratch provides an appealing graphical environment aimed at introducing programming to 8-16 year olds. Familiar programming constructs like conditionals and loops are present, but are presented in an intuitive manner. The authors hold that the ``computational thinking'' that programming affords is a valuable tool that should be encouraged but is currently lacking in non-Computer Science curriculum. Most children's digital fluency is defined by ``reading'' but not ``writing.'' The article discusses some of the design decisions that went into the language. To make it appeal to their target audience, the authors wanted Scratch to be more tinkerable, meaningful, and social. Additionally, we can think of any programming language as having a ``low floor'' (easy to begin), ``high ceiling'' (ability to become more complex), and ``wide walls'' (support for a diverse range of projects). Scratch appropriately focuses on the low floor and wide walls, arguing that a higher ceiling is outside the scope of a learning language, and if students are interesting in ``raising the ceiling'' they can and should look to more established languages like C or Python \cite{resnick}.

Another interesting paper draws parallels between teaching a second natural language and teaching a first programming language. The authors claim that these processes are not as different as one might expect. While much research has gone into how best to communicate the syntax and semantics contained in a natural language, teaching programming languages is sorely in need of such instruction. Central to that argument is a concentration on writing code over reading it. A large and important part of learning a natural language is an ability to read, digest, and internalize the information being presented. So, the authors write, we must teach our students to read the code as well as write it. We note that this comes as a contrast to the MIT Scratch approach, which focuses on creation. Clearly, both aspects of a language are important. We need a language that is both intuitive to write and readable for others to interpret \cite{robertson}.

We now turn to work regarding CUDA. Most of an undergraduate's computer science training is involved with writing single-threaded, sequential code. Since single-core CPUs are reaching limits on a hardware level, a shift is underway towards multi-core machines. This paper describes how to harness a GPU for general purpose computing using CUDA and how one might implement this into a college curriculum. A high-level overview of the CUDA model is presented along with a sample algorithm that illustrates the usefulness of teaching this new paradigm. The most useful part of this paper was the overview of the process that goes into writing a CUDA program. Given a bird's-eye view of the CUDA model, we can begin to get an idea of how to shape our own language. For example, in CUDA the programmer must decide how many threads will be executed. We would like to hide this detail from the user since it is too low level. Given this kind of big-picture writeup, we can begin to see what may cause problems and what patterns may emerge \cite{tran}.

Finally, we need to consider the paradigm of actually programming for a graphics processor. Converting existing programs from a singly threaded model to the CUDA model can take significant time and experience, partly because of the mechanical conversions needed such as computing array references instead of using loops. hiCUDA is an example of an OpenMP style C preprocessor language that allows the programmer to wrap his singly threaded loops to allow them to be automatically parallelized by CUDA. It was found that examples written with the prototype hiCUDA implementation did not perform significantly worse than the same examples written directly with CUDA. Additionally, the hiCUDA examples were more concise and better fit into existing loop-based computation models that are often present in single threaded programs. However, the language is still based around C using compiler directives and as such may not reflect or emphasize the models that one must internalize to understand GPGPU programming. Still, hiCUDA shows a similar abstraction-minded attitude and gives us an idea of what can be accomplished when building on top of CUDA \cite{han}.

\section{General Purpose \\ Graphics Processing}
\label{sec:GPGP}

\subsection{Structure of Graphics \\ Processing Unit}

A graphics processing unit is a dedicated piece of hardware separate from the CPU. It was originally designed for graphics processing where each pixel is run through a series of transformations and filters before it is shown on the screen. However, the massively parallel architecture of the GPU is now being used for more general purpose programming. 

Modern GPUs are structured with dozens of individual processing units organized into blocks on the card. Each of these blocks has access to a small amount of fast block-shared memory (similar in speed to the cache on a CPU) and the vastly slower but much larger GPU main memory. Unlike CPUs, GPUs can not assign and run different threads of computation simultaneously on their many processing units. Instead, they run a single process (called a kernel) on every unit. These kernels are able to access both types of memory and perform their computations in parallel. The programmer is responsible for writing this kernel, as well as for designing the way in how threads will be assigned data. This can be a complex task that requires some low level knowledge on how the GPU is designed as well as knowledge of special extensions to a programming language (e.g. CUDA or OpenCL) that can tell the GPU what to do.

\subsection{Paradigm/Data Flow}

GPUs lend themselves to certain types of programming, and these are not always the same as what traditional parallel models offer. For chestnut we have focused on a single paradigm that allows for significant and semi-automatic parallelization. This model is data oriented and focuses on the interactions between functions and data. Large chunks of data are assumed, such as large 2d arrays where each element in the array is a scalar value. Functions are written to perform the same operation on each element of data with no ordering constraints. This means that the operation can be applied to each element in any order, and more importantly means that the operations can be applied in parallel. This model of programming maps quite nicely onto the core parts of GPU programming. A single program can be written as a kernel and each running instance of that kernel can operate on a small chunk of the overall data.

\subsection{CUDA}

Compute Unified Device Architecture (CUDA)\footnote{\url{http://www.nvidia.com/object/cuda_home_new.html}} is a system developed by NVidia for performing general purpose computations on the GPU. CUDA is a series of extensions to C along with a specialized compiler. This results in a very low level language where the programmer must know about the GPU to program for it. Because of the extensions to C and amount of fine-grained control that CUDA offers code written in it ends up being verbose and difficult to understand for someone who has not learned about how graphics cards work. This creates an enormous learning curve for aspiring GPU programmers.

\subsection{Thrust}

%Much more C++-y. Easier to grasp. More like programming for CPU but still need to know some stuff about GPU

Thrust is a C++ template library that provides convenient, optimized abstractions for many common CUDA operations. Where CUDA is very low-level, verbose, and C-like, Thrust operates on a higher level, is more succinct, and much more closely resembles C++ code. Thrust makes an introduction to GPU programming much more feasible compared to CUDA. However, Thrust code is still more verbose than we would like, is specific to C++ -- a python coder would be lost -- and requires knowledge of the GPU. A Thrust coder still needs to know that copying memory between CPU and GPU is very slow. Using this knowledge, there are some optimizations that one can use in Thrust to speed up the execution of code. We want to use many of the paradigms that Thrust expressions while simplifying the process such that a user will not need a deep understanding of the GPU framework \cite{hoberock}.

\subsubsection{Vectors}

There are two types of vectors in Thrust. The first is the \code{thrust::host\_vector}\footnote{We use the \code{thrust::} notation when introducing a Thrust object to distinguish Thrust code from Chestnut code}, which is stored in the CPU memory. The other is the \code{thrust::device\_vector}, living on the GPU. In general, a user can only initialize a vector on the CPU and then must transfer it over to the GPU. These vectors are very similar to the STL C++ vectors, and as such are inherently one-dimensional. In Chestnut we view our data in two-dimensions, but we abstract this difference away so the user never concerns himself or herself with the conversion. 

If the user wants to fill a vector full of the same value, Thrust offers the \code{thrust::fill} function, which fills values directly into the device vector without the need to copy data over. Instead, \code{fill} calls a kernel on the GPU that sets each index in the vector to the specified value. This is significantly faster than copying from CPU to GPU. There also exists a special type of iterator called a \code{thrust::constant\_iterator} that takes only one unit of space. The user ``fills'' it with some constant, and a query at any index returns that value. For example, suppose a user created a \code{constant\_iterator} called \code{iter} with value 2. Making the query \code{iter[0]} would return 2, as would \code{iter[100]} or any other index. Since constant space is reserved for any ``size'' vector, using  the \code{constant\_iterator} is a great optimization that we have bestowed on Chestnut users so they don't have to learn the quirks of a language like Thrust. 

\subsubsection{Functions}

Thrust offers a large library of default functions. For this project we have chosen to focus on three in particular: \code{thrust::transform}, \code{thrust::reduce}, and \code{thrust::sort}. These three functions lay the foundation for manipulating data in Chestnut. We briefly introduce them here before delving into the translation from Chestnut code to Thrust code in section \ref{sec:chestnutToThrust}. The \code{transform} function convolves two vectors using a specified operator. \code{Reduce} consolidates a vector with a given operator. Finally, \code{sort} sorts the data with a given comparator. Each of these operators and comparators can be chosen from one of several provided by Thrust or they can be customized. At present, Chestnut uses just the default operators and comparators. For a discussion specifying how Chestnut implements these functions, see section \ref{sec:manipulators}.

\section{Chestnut}
\label{sec:chestnut}

\subsection{Overview}

Chestnut is a graphical environment for parallel programming. It is composed of a graphical frontend, an intermediate language and a compiler that translates the intermediate language into thrust C++ code. The main point of entry for users is imagined to be the Chestnut GUI, with the simple code underlying this to be a small but more complicated step towards actual thrust code. Chestnut is based on the data-oriented programming model that was explained earlier. This is enforced through the gui and through the limited syntax offered by the underlying program.

\subsection{Target Audience}

Chestnut is meant to be a relatively simple introduction to parallel GPU programming. We expect little to no experience with C++ and see two categories of users with this type of knowledge. The first group is computer science/programming students who have not had any exposure or introduction to GPU programming before. The second group is non computer science programmers who could benefit from parallelizing their computations. Examples of the latter might be other natural science researchers who have written number-crunching programs in scripting languages who have parallelizable algorithms.

With both of these cases the learning curve can be too steep to encourage these programmers to learn about GPU programming. With the underlying chestnut code we are interested in having a similar syntax to python or javascript, where the code is similar to C but without some of the extra syntax such as namespaces, headers vs source files, includes and other complexities. We hope that a person with some knowledge of programming and with a problem could use Chestnut to write a basic solution to this problem.

\subsection{Target Applications}

At this point Chestnut is targeted towards embarrassingly parallel problems. We reason that there are a vast number of embarrassingly parallel problems out there that have not been written to take advantage of the GPUs because of the learning curve associated with GPU programming. Those are our target problems. Additionally, because the target audience is not advanced computer science programmers we are not convinced that the extra complexity that would be needed by offering things like more fine-grained synchronization, lower level kernel specification and control over how blocks of threads are distributed is warranted.

\subsection{Goals}

With both the target audience and the target applications in mind we have identified a few important goals for Chestnut. Primarily it should expose the data-focused model that can be translated to the GPU easily. This can be done by imposing limitations on the gui and on the core language syntax. Secondly we want Chestnut to be modular. This means separating the gui interface from the underlying language, and providing a compiler for this underlying language that could be changed in the future to not depend on Thrust or CUDA should either become eclipsed by other GPU programming environments. Chestnut needs to be discoverable. The gui frontend and its visual and drag and drop interface is a direct result from this, where the user can see how things are supposed to be connected. Lastly we are still interested in achieving a speedup when compared to the CPU. Although Chestnut is not about the fastest possible runtime on the GPU compared to other GPU programs we still want it to be faster than running the same computations on the CPU because even with chestnut there is still a learning curve to working on the GPU.

\section{Chestnut Frontend (AS)}
\label{sec:chestnutFrontend}

The Chestnut frontend is designed with the data-centric programming model in mind. It is composed of a canvas upon which objects can be placed and connected together to represent the dataflow within a program.

\subsection{Primitives}

The frontend has three main types of objects, of which two are data containers. The simplest data container is a \code{value}, composed of just a single scalar. \code{Values} are used primarily for the second input to a map function or the result calculated by a reduction function. \code{DataBlocks} are data containers that contain multidimensional data. Currently, only 2d arrays of arbitrary dimensions are implemented, but this could be expanded to three dimensions. Lastly there are \code{functions} which operate on the two data containers. \code{Functions} are well specified and strongly typed; for example, the map function requires two inputs: one DataBlock and either another DataBlock or a Value. The map function then outputs a DataBlock. Functions (such as map and reduce) can optionally accept an operation type which takes two scalars and combines them (for example + or *). With these three types of primitive objects and the connections between them we think many embarrassingly parallel problems can be expressed in Chestnut code.

\subsection{Interface Concepts}

\begin{figure}[h!]
  \centering
    \includegraphics[width=100pt]{simplegui}
  \caption{A sample program in the chestnut GUI.}
\end{figure}

With Chestnut one of the main goals was to have a discoverable interface. We've taken a number of steps towards this end. Firstly, each class of objects has a consistent and unique shape. \code{Functions} are rounded rectangles, \code{DataBlocks} are sharp-cornered rectangles and \code{Values} are triangles. This gives a the user a visual reference to the type of the object he is looking at rather than having to parse and understand some text. Objects can have inputs and outputs, known as \code{sinks} and \code{sources}. A sink corresponds to an input. It can accept data from some source. A \code{source} is like a fountain of data, produced by some object and available to connect to an arbitrary number of sinks. Continuing with the theme of a discoverable interface, sources and sinks have shapes to represent how they can be connected; circles can only be connected to other circles, triangles to triangles and so on. Similarly, sinks are differentiated from sources by a darker interior color. A sink can not be connected to another sink, nor can a source be connected to another source. It is possible to have a sink accept multiple different types of sources. For example, the print function can take either a DataBlock or a Value. This is represented by having both a triangle and a circle next to each other. To help the user create acceptable connections between objects, the Chestnut frontend specifically prevents the user from doing thinks that are impossible, such as connecting a source and a sink of incompatible types or connecting two sources to one sink.

The drag and drop paradigm is central to the Chestnut GUI. Objects are placed on the canvas by dragging them from the left toolbar, connections are made and destroyed by drags from the respective sinks and sources, and objects can be rearranged by dragging them around the canvas. We decided on this drag and drop interface because of the goal of a discoverable frontend. 

The Chestnut frontend forces the data-centric GPU model that we illustrated earlier because of the limitations enforced with connections and the high level atomic functions such as sort, reduce, and map. Sort and reduce are both predefined functions with readily available parallel solutions, such as those present in Thrust. Map enforces our model because it operates on each chunk of the data and performs the same operation to each chunk.


\subsection{Translate GUI to Chestnut Code} 

%Visit each node. Each node knows its contribution. Variable Declarations and Function calls.

In order to translate the graphical representation of a given program to Chestnut code that we can translate into Thrust code we need a robust method of traversing the graph and constructing correct code. It is difficult to decipher a ``starting point'' in a graphical program, so we start by choosing a node at random from the graph. This node may be data or it may be a function. Each node knows its contribution to the code and also knows what sinks and sources it is connected to. Additionally, every node has a \code{flatten()} function that builds up the Chestnut code. Thus, once we have selected our starting node, we call its \code{flatten()} function which in turn calls the \code{flatten()} function of all its sinks, then provides its contribution to the Chestnut code, and finally calls the \code{flatten()} function of all its sources. Since all sinks are ``flattened'' before the node's contribution is added and all sources are ``flattened'' after that point, we ensure that the order of commands in the resulting Chestnut code is as the GUI layout intended.

We have so far been vague about what a node does to provide its contribution. This of course depends on the node's type and its specific function. But first we need to discuss the structure of Chestnut code as it is being built up. Every Chestnut program can be broken up into variable declarations and function calls (for Chestnut code specifications see section \ref{sec:specifications}) and so we can create two lists of strings, one for declarations and another for calls, that will eventually be written out to disk in the Chestnut code file. Thus as a node contributes to the list of strings, it adds to the declarations or calls. 

So, when a \code{DataBlock} or \code{Value} is encountered its contribution will be to the variable declaration list. The node uses the context provided by a presence or lack of sinks to decide whether or not it has been initialized and adds an appropriate variable declaration (see section \ref{sec:iofunctions} for a description of variable declarations in Chestnut code). If \code{flatten()} is called on a \code{Function}, the contribution will be to the function calls. As with the \code{DataBlock} the node can gain context from its sinks and sources, allowing it to add the appropriate parameters to the Chestnut code string.

In choosing a random node from the graph and traversing it as we are, we make two crucial assumptions. The first is that any connections a \code{Function} has will either me a \code{DataBlock} or \code{Value}. Similarly, any connections to or from a variable must be a \code{Function}. This decision reinforces the data-centric model of pushing data through functions and getting data back. We can enforce this at the graphical level by not allowing two \code{Functions} or two variables to be connected. The second assumption is that the graph is connected -- i.e.\ there are no disjoint subgraphs. Since we choose a node at random and traverse the graph from there, we can only translate code that is reachable from that initial node. Thus at present we don't guarantee any correctness if disjoint subgraphs (including single, unconnected nodes) exist.

\section{Chestnut Backend}
\label{sec:chestnutBackend}

Once the GUI to Chestnut translation is finished, we have Chestnut code output. In this section we discuss the process that brings our Chestnut code to executable object code.

\subsection{Process}

%Lex tokenizes, Yacc accepts sequences of tokens. From Yacc we call helper functions which write Thrust code to disk. Compile that code using nvcc
The translation from Chestnut to executable is a four step process. First, we use Flex\footnote{An open-source version of Lex, found at \\ \url{http://flex.sourceforge.net/}.} to tokenize the Chestnut code. For each valid word in the language, a token is created. For example, if Flex reads the word \code{map} it outputs the \code{TOKMAP} token. While Flex can return just a token without any extra context, it also has the ability to recognize and then save the input in a string or integer and pass that information along as well. In this case, one might have the variable \code{var1} which gets mapped to the {\em identifier} token \code{ID} and additionally saves the string \code{"var1"} to pass along to the next stage. Every valid word or character in the file (including braces, parentheses, etc.) must be tokenized to be accepted and passed along. If a token is not found, Flex will exit with a syntax error. This provides a simple but effective first level of error checking for our code.

As Flex tokenizes the file, it passes each token along to Bison\footnote{The GNU version of Yacc, found at \\ \url{http://gnu.org/software/bison/}.} for processing. Bison generates an LALR(1) (one token Look-Ahead LR) or a GLR (generalized LR) parser. Using very simple regular expressions, Bison generates code that analyzes each token and matches it to predefined acceptable sequences of tokens. For example, suppose the input code for a reduction was \code{"result~=~reduce(data)"}\footnote{See section \ref{sec:specifications} for a full description of syntax}. The resulting sequence of acceptable tokens to check for would be \code{"ID~ASSIGN~TOKREDUCE~LPAREN~ID~RPAREN"}. Once a valid sequence of tokens is identified, we gather the appropriate information (variable names, operators, parameters) and pass them into a utility function (part of a utility class) that takes care of writing all the necessary Thrust code to disk. Once Thrust code has been written it can be compiled to object code using the CUDA compiler, \code{nvcc}.

The utility class knows what files we eventually want to write to, keeps a symbol table, and has all the necessary functions to write appropriate Thrust code. We have broken each file into four general regions: headers (\code{\#includes}), function declarations, the \code{main} function, and function definitions. Each region is represented as an STL vector of strings. Thus, any header files we want to include in a given file or code we want to write to \code{main} get stored as entries in their respective vectors until the Chestnut code is finished being parsed. Once our parser reaches the end of the file, we execute a single write to disk.

The symbol table is implemented as an STL vector of \code{symbol\_entry} structs. A \code{symbol\_entry} is a simple container for four fields. The first is the name of the function or variable. The second is that object's type, either \code{int} or \code{float} at present (see section \ref{sec:specifications} for a complete discussion of supported types). The third field is the category of the entry. It can be either a \code{FUNCTION} or two different variable types. If the variable is a single value, we assign the category \code{VARIABLE\_SCALAR}. If the variable is a vector (or DataBlock, in the frontend terminology) we assign it \code{VARIABLE\_VECTOR}. Finally, each entry has a scope. This field is not currently being used and is by default set to zero. 

\subsection{Specifications}
\label{sec:specifications}

%Support ints, floats. Functions: map, reduce, sort, print, read, write. Ask Tia if we should put specifications here or in an appendix or what.
We currently support two basic variable types, two variable categories, and seven default functions. Here we discuss them and specify each of their uses. This section will introduce the reader to basic Chestnut syntax and give the reader a sense of the level of abstraction used.

At this early stage, the only data types we support are standard C++ \code{int} and \code{float}. Considering our target audience and what we anticipate them using Chestnut for, we believe that in an overwhelming number of cases these basic number types will be sufficient. Users will be acting primarily on large numerical datasets and our language absolutely supports that. In the future, we would like to implement more default types and allow users to define their own types, described in section \ref{sec:future}. 

Before a function is called, variables used by that function must be defined. As discussed earlier, variables can be either vectors or scalars. If a vector is going to be assigned data before being used in a function, it must be initialized using a \code{foreach} loop or a \code{read} command, discussed in section \ref{sec:iofunctions}. Otherwise, if data is going to be used as the result of a function, we must still declare it, but need not specify anything except its type, name, and category. To declare a variable in this way, the general syntax is
\begin{center}
  \code{[type] variableName [category];\footnote{Bracket notation is used to indicate there a defined set of choices for the given identifier. For example, \code{[type]} could be \code{int} or \code{float}.}}
\end{center}

Chestnut currently has seven functions that operate on data: \code{map}, \code{reduce}, and \code{sort} all manipulate the data in some way; \code{foreach}, \code{read}, \code{write}, and \code{print} comprise the Input/Output interface of our language. The general syntax for a function call in Chestnut is
\begin{align*}
  \code{result } &\code{= functionName(} \\ &\code{$param_1,\;\ldots, \;param_k,$ } \\ &\code{inputData);}
\end{align*}
The result of the operation is always stored in a variable (either a scalar or vector) to be used later. Every function also operates on some block of data. These two specifications enforces the data-centric model of programming. Note that it is easy to modify data in place by setting both the result and input to the same variable. Each $param_i$ is an extra parameter that specifies some option of the function. There can be zero or more such parameters. 

\subsubsection{Manipulator Functions}
\label{sec:manipulators}

Let's consider the \code{map} function, the most complex of the default functions. A \code{map} takes a block of data and applies the same modification to every element. For example, we could add 2 to every element in an array. Our \code{map} also works as a convolution operator, applying an operator to corresponding elements in two arrays. Formally, given two 2-dimensional arrays \code{A,B} convolved into a resulting array \code{C}, each with indices \code{i,j}, the resulting array would have the property \code{C[i][j]~=~A[i][j]~+~B[i][j]} for all \code{i,j}. The result of a \code{map} is a vector. The first parameter is the operator (e.g.\ \code{"+"}). The second parameter can either be a single value (e.g.\ \code{"2"}) or a data block. The last parameter is, as always, the input data that will be mapped over. So, if the end user wants to modify a data block \code{inputData} by adding 2 to every element the function call would be
\begin{center}
  \code{inputData = map(+, 2, inputData);}
\end{center}
If instead the user wants to convolve two blocks of data, \code{input1} and \code{input2}, using the multiplication operator and store the result in a third variable \code{output}, the call would be
\begin{center}
  \code{output = map(*, input1, input2);}
\end{center}

The \code{reduce} function accumulates every element of a data block using a specified operator and stores the value in a scalar. For example, a user could use the function to take the sum of every element in an array. The function takes only two parameters, an operator and an input vector. The output is a scalar. Using the example from before, to sum over a data block \code{input} and store the result in the scalar \code{reduced}, we have the syntax
\begin{center}
  \code{reduced = reduce(+, input);}
\end{center}

The \code{sort} function sorts a block of data according to the given comparator. The output is always a data block. The first parameter is the comparator to use (e.g.\ \code{"<"}) while the second is the data to be sorted. Suppose a user wanted to sort a block of data \code{input} in descending order and wanted to store the result in that same variable, the syntax is
\begin{center}
  \code{input = sort(>, input);}
\end{center}

We see that each of the manipulator functions Chestnut offers takes a vector as input, performs an operation on every element and returns some value to be stored in another variable. We note that these functions can be strung together into long sequences of maps, reduces, and sorts. Indeed, we anticipate much of the value of Chestnut to arise from the ability to sequence functions in a pipeline to efficiently compute results.

\subsubsection{Input/Output Functions}
\label{sec:iofunctions}

To initialize a vector, Chestnut offers two options, one of which is the \code{foreach} construct. This is a simplified for-loop construction that traverses every element of the vector and allows the user to create a formula to dictate what value is stored in each cell. The general syntax for a \code{foreach} declaration is
\begin{align*}
  \code{[type]}& \code{ variableName numRows numCols } \\ &\code{foreach ($ForeachExpression$);}
\end{align*}
Every $ForeachExpression$ starts with \code{value =} and then some right-hand side expression. The \code{value} keyword is shorthand for the current index of the data block in the for loop. Thus if the progress of the loop had reached $currentRow$, $currentCol$, then \code{value} would correspond to \code{array[$currentRow$][$currentCol$]}.

The right-hand side of the equation can be any arithmetic expression (e.g.\ 2+4) combined with a set of reserved words. Note that these words are not reserved outside of this context, so a user oblivious of these rules would not be in danger of name conflict errors. The user can reference the current row or column using the \code{row} and \code{col} keywords, respectively. To reference the total rows or total columns a vector has we use \code{maxrows} and \code{maxcols} keywords, respectively. Finally, users can use the keyword \code{rand} to generate a random number. Since this function invokes the C++ $rand()$ function, this will be an integer between 0 and the maximum random number. This offers the most ability for the user to customize his or her random number. 

So, to fill a 10x10 vector of random real numbers between 0 and 1, the full statement would be
\begin{align*}
  \code{float } &\code{data 10 10 } \\ &\code{foreach (rand/RAND\_MAX);}
\end{align*}

Chestnut also allows its users to read data from disk and write data to disk. We use a very clear, simple syntax to read and write files. The first line contains the number rows in the data, then the number of columns. After this the data is listed in a space-separated format. Recall from above that data reads are called only when variables are declared, so the current syntax follows the declaration pattern. In the future we may want to allow data reads at any time. To read from a file \code{"infile"} and store it in a vector \code{data} of integers, we have the syntax
\begin{center}
  \code{int data read("infile");}
\end{center}

The output functions have a different syntax from other functions. To write a block of data, \code{outdata}, to a file, \code{outfile}, the code in Chestnut would be
\begin{center}
  \code{write(outdata, "outfile");}
\end{center}

Similarly, Chestnut provides a function to pint the contents of a data block to standard output. The function takes the rows and columns of the data into account when printing, printing a newline character before starting each new row. To print a data block \code{outdata}, one uses the syntax
\begin{center}
  \code{print outdata;}
\end{center}

The input and output functions Chestnut provides allows the user to view progress in between a series of computations and allows the user to take blocks of data from disk and write them out for storage. When dealing with enormous blocks of data, it seems that reading and writing to disk will be most useful. Still, for benchmarking and various other operations, the \code{foreach} construct will also be useful.

\subsection{Sample Program}

%Do we want to show some samples? Use verbatim!

We have provided in-depth specifications of Chestnut code but have not yet given an example of a full program written in the language. Here we take the opportunity to detail a program written in Chestnut code from start to finish, given in Code Snippet \ref{code:fullprog}.

First, we initialize \code{input1} and \code{input2} with a random number and data from file, respectively. We then need to declare two vectors, \code{sorted1} and \code{sorted2}. Below the declarations, we sort the initial vectors into their sorted analogues, the first in ascending order, the second in descending order. To get a peak at the computation in progress, we print out both vectors.

Once the data are sorted, we want to convolve over the vectors using the \code{"*"} operator and store them in \code{mapped}, which we then write out to the file \code{"outputdata"}. Finally, we reduce over a sum and print out the result, which is stored in \code{reduced}. This sample program shows the ease with which a user can quickly populate and operate on sets of data. There is also a clear paradigm at work, moving data through functions and storing the results in more data.

%\floatstyle{boxed} 
%\restylefloat{Code Snippet}
\begin{Code Snippet}
\begin{verbatim}

   1   float input1 100 100 foreach 
               (value = rand/RAND_MAX);
       float input2 read ("inputdata");
   
       float sorted1 vector;
       float sorted2 vector;
   5   sorted1 = sort(<, input1);
       sorted2 = sort(>, input2);
   
       print sorted1;
       print sorted2;
   
       float mapped vector;
   10  mapped = map(*, sorted1, sorted2);
       write (mapped, "outputdata");
     
       float reduced scalar;
       reduced = reduce(+, mapped);
   
   15  print reduced;
\end{verbatim}
\caption{Sample Chestnut code for a program that adds two sorted vectors together, then sums over their mapped result. Note that line numbers do not include blank lines or wraparound. We do this to focus on the lines of actual code rather than the code's organization.}
\label{code:fullprog}
\end{Code Snippet}

\subsection{Translating Chestnut Code to Thrust}
\label{sec:chestnutToThrust}

%Talk about the assumptions we need to make (when/where memory is allocated), some optimizations (\code{constant\_iterators}), the basic Thrust functions used.

The process of converting Chestnut Code to Thrust code consists of expanding single lines into many lines of code, declaring extra variables that are encoded implicitly in Chestnut, and making some assumptions that a Thrust programmer would not need to. In certain circumstances, we can also optimize some commands. In this section we discuss Trust code that is produced from samples of Chestnut code. We also use this as a vehicle to discuss other design decisions and assumptions we made regarding the translation.

\subsubsection{Variable Declarations}

Any time a variable is declared in Chestnut there up to five possible variables that may need to be declared in Thrust. Consider a vector defined using the syntax \code{int data vector;} which just creates a vector for later use. This causes a utility function to create a host vector and a device vector along with variables to contain the number of rows and columns of the object. None of these variables are initialized immediately, but they must be declared to be referenced later in the program. The Chestnut code \code{int data scalar;} corresponds to just a single variable in Thrust code.

When translating a \code{foreach} construct, we must create host, device, rows, and columns, but we still have two options after those declarations. If the user specifies a row, column, or the random keyword in the $foreachExpression$, then we must create a double for-loop that iterates over every row and column and fill the host vector, then copy the host vector to the device. But if the user makes no reference to those reserved words we can optimize the process by using the \code{thrust::fill} function. Since all other expressions (those containing \code{maxrows} and \code{maxcols} and all arithmetic expressions) are constants, we can simply pass the expression to the function and have it move straight to the GPU. Since there is no need to copy data from CPU to GPU, this operation is much faster.

When the user calls the \code{read} function, we must create the usual four variables and then must additionally reference a special \code{DataInfo} struct. In the Thrust code, we call a templated library function we wrote that reads the data into a host vector, fills out row and column information and packs them into a special struct. This is then returned to the Thrust file where it gets unpacked.

With so many variables in the Thrust code, we need a convenient naming scheme. The base of every Thrust variable name is the name given from Chestnut. Then some descriptive suffix is added to enable us to reference the variable and to ensure there are no naming collisions. Thus, if a Chestnut vector was \code{input}, the associated host vector in Thrust would be \code{input\_host} and the number of rows would be \code{input\_rows}.

\subsubsection{Manipulator Functions}

%Interesting part comes when you think about memory access $\rightarrow$ different sense of an ``object'' in Chestnut and Thrust.

Where conflicts in name collisions arose when declaring variables, conflicts on function calls come from a mismatch between the rigidity of Thrust (and C++) and our goals of making the language as intuitive as possible. Every function in Chestnut has an input and an output, and unless those two are the same object we guarantee that the function will not modify the input object. But some functions in Thrust modify the data in place. For example, if a user writes the Chestnut code \code{output~=~sort(<,~input)}, we must copy the contents of \code{input} to \code{output} and then run an in-place sort on \code{output}. 

Similarly, when using the map function, a conflict arises when the user sets both the modifier data block and the output as the same object, as in \code{modifier~=~map(+,~modifier,~input)}. Since we want to guarantee \code{input} is not modified, we set the destination (here, \code{modifier}) to an empty vector of the correct size. Thus, left unmodified, the translation results in \code{modifier} being overridden and then simply filled with the \code{input}. Here we need only switch the order of the input and modifier to resolve the problem, because then both input and destination will be the same object and our algorithm knows that this is an in-place modification. These problems are indicative of the level of abstraction we strive for. General purpose graphics processing requires a different way of thinking about the data, and to really establish and enforce this paradigm requires some bridges between existing code structures and our abstractions.

\subsubsection{Output Functions}

With the output functions \code{write} and \code{print} we come to one of the most crucial assumptions. To write a data block out to disk or to print an object, the data must be on the CPU memory. Since the Chestnut programmer is not informed about the CPU/GPU boundary and by design has no way of affecting the location of the data, we must assume that the most recent copy of the data is on the GPU and thus need to copy it over to the CPU on every output function call. Thus every print and write, even if they are consecutive calls, necessitates a slow copy. Future work could look at optimizing this limitation out. 


\section{Experiments}
\label{sec:experiments}

%Run both qualitative experiments -- compare code samples -- and quantitative experiments with runtimes. We show that, subjectively, our code is easier to read, if less powerful. BUT!\ runtimes are very similar.

Experiments with Chestnut take two forms: qualitative and quantitative. The first allows us to evaluate the readability of code. We compare the Chestnut code automatically translated from the graphical interface against equivalent Thrust code and CUDA code. We use this evidence to argue that our language is much more accessible than the alternatives. The quantitative experiments help to validate the usefulness of Chestnut. We show that Chestnut code is significantly faster than sequential code executed on the CPU and is approaching the speed of handwritten CUDA code.

\subsection{Qualitative}

%Compare Chestnut to Thrust to CUDA
To demonstrate the readability and ease of writing in Chestnut, we present a simple example involving a single \code{map} operation and show how the verbosity increases dramatically as we translate the code into equivalent Thrust and then into CUDA code. We see in Code Snippet \ref{code:chestnutExperiment} that in three commands the user can define an array, fill it with a value, add one to every element in that array, and print out the result. The commands are clear, succinct, and immediately accessible to a new user. 

\begin{Code Snippet}
\begin{verbatim}

   float data 10 10 foreach (value = 2);
   data = map(+, 1, data);
   print data;
\end{verbatim}
\caption{Chestnut code mapping over each element of a 10x10 array, adding 1 to each element, and printing out the result.}
\label{code:chestnutExperiment}
\end{Code Snippet}

Let us contrast the Chestnut code with equivalent Thrust code. Code Snippet \ref{code:thrustExperiment} contains the result of automatic translation from Chestnut to Thrust code, formatted slightly to fit the page. The first feature to notice is how many more lines of code are necessary to do the same thing. In the Chestnut code, we need only declare the variable and use it without worrying about about whether its contents reside on the CPU or GPU. That abstraction disappears when we begin to program in Thrust. First host and device vectors must be declared and populated. Then a verbose \code{transform} function must be called wherein the user must worry about bounds and optimizations like the \code{constant\_iterator}. Finally the data on the GPU must be copied to the CPU and printed out. 

\begin{Code Snippet}
\begin{verbatim}

  1    int main() {
         int data_rows = 10;
         int data_cols = 10;

         // Memory on host and device
  5      host_vector<float> data_host; 
         device_vector<float> data_dev
             (data_rows*data_cols); 

         // populate data
         thrust::fill(data_dev.begin(),
             data_dev.end(),  2);

         /* Begin Map Function */
  10     thrust::transform(data_dev.begin(), 
             data_dev.end(), 
             thrust::make_constant_iterator(1), 
             data_dev.begin(), 
             thrust::plus<float>()); 
         /* End Map Function */

         // print out data
         data_host = data_dev;
         for (int r=0; r<data_rows; r++){
  15       for (int c=0; c<data_cols; c++){
             std::cout
               << data_host[r*data_cols+c]
               << " ";
           }
           std::cout << "\n";
         }
  20     std::cout << "\n";

         return 0;
  22   }
\end{verbatim}
\caption{Thrust code mapping over each element of a 100 element array, adding 1 to each element, and printing out the result.}
\label{code:thrustExperiment}
\end{Code Snippet}

But Thrust is still a strong improvement in terms of usability over the low-level CUDA code. In Code Snippet \ref{code:cudaExperiment} we see handwritten code equivalent to the previous two programs. While there are not markedly more lines of code in the CUDA program compared to the Thrust program, each line is nearly indecipherable for a user not well versed in C programming. Among the things a CUDA programmer needs to worry about are pointers, using CUDA's set of memory allocation tools, and running a kernel that uses obscure syntax. Recall the simple task we're accomplishing with each of the programs discussed. All we want to do is add 1 to a small array. We believe that such an easy to understand concept should have accompanying code that is both equally easy to understand and easy to program. We have shown that Chestnut makes this a reality.

\begin{Code Snippet}
\begin{verbatim}
 
    1   int main() {
          int* host;
          int* dev;
          int N = 100;
     
    5     cudaMallocHost((void*)&host, 
             N*sizeof(int));
          cudaMalloc((void*)&dev, 
             N*sizeof(int));
      
          for (int i=0; i<N; i++){
            host[i] = 2;
          }
 
    10    // Copy the array host to dev
          cudaMemcpy(dev, host, 
             N*sizeof(int),
             cudaMemcpyHostToDevice);
 
          // run map kernel on device
          map<<<1, dim3(N)>>>(dev, N);
 
          // copy back dev to host
    15    cudaMemcpy(host, dev, 
             N*sizeof(int),
             cudaMemcpyDeviceToHost);
      
          for (int i=0; i<N; i++){
            printf("%d ", host[i]);
          }
          printf("\n");
      
    20    return 0;
        }
 
        __global__ void map
           (int* array, int cols){
    25    int row = threadIdx.x;
          int col = threadIdx.y;
       
          array[row*cols + col] += 1;
    28  }                                    
\end{verbatim}
\caption{CUDA code mapping over each element of a 100 element array, adding one to each element, and printing out the result}
\label{code:cudaExperiment}
\end{Code Snippet}

\subsection{Quantitative}

%Runtimes.
An important step in determining whether Chestnut is a viable alternative to other general purpose graphics processing languages is benchmarking. We ran our automatically generated Thrust code against handwritten sequential code running on the CPU and optimized, handwritten CUDA code. We tested \code{Map}, \code{Reduce}, and \code{Sort}. The first two operations were performed on arrays containing 8192x8192~=~67,108,864 elements. The sorting operation ran on an array of size 1024x1024~=~1,048,576 elements. In all cases, arrays were initialized with random floats between 0~and~1. Each operation was executed 100 times per run. The average runtime over five runs is presented in Table \ref{table:runtimes}. Additionally, speedups between sequential and Thrust code and between Thrust and CUDA code are featured.

\begin{table*}
  \centering
\begin{tabular}{| c || c|c || c|c|c |}
  \hline
                 & Sequential to Thrust & Thrust to CUDA & Sequential & Thrust & CUDA \\
                 &       Speedup                   &       Speedup             &    (ms)   &  (ms) & (ms) \\ \hline \hline
  \code{Map}     &  {\bf 23.1}                     & {\bf 1.01}                & 545       & 24.3  & 24.1 \\
  \code{Reduce}  &  {\bf 19.2}                     & {\bf 58.0}                & 266       & 13.9  & 0.24 \\
  \code{Sort}    &  {\bf 13.7}                     & {\bf 3.32}                & 409       & 29.8  & 8.97 \\ \hline
\end{tabular}
\caption{Runtime statistics comparing automatically generated Thrust code to Sequential code run on the CPU and optimized CUDA code. Map and Reduce were run on arrays containing 67,108,864 elements while Sort was run on 1,048,576 elements. Each operation was executed 100 times per run and the average runtime per operation is presented in milliseconds. Speedup in the first column is calculated as $\frac{sequential\; time}{Thrust\; time}$, the second column is $\frac{Thrust\; time}{CUDA\; time}$.}
\label{table:runtimes}
\end{table*}

We see from the results that in all cases we have excellent speedup compared to sequential code. Our generated Thrust code is anywhere from 13 to 23 times faster than handwritten sequential code. When we look at the Thrust code against optimized CUDA code, we in general fare respectably. When considering the map and sort operations, our code is at most three times slower. The reduction operation, however, is almost sixty times slower than the CUDA code. It would appear that the default Thrust \code{reduce} operation is not nearly as optimized as it could be. We know that the \code{reduce} function requires copying the result from GPU to CPU memory and it may be the case that the optimized CUDA is getting around that. While we are satisfied with the \code{map} and \code{sort} speedups, we need to look more carefully about how to handle reductions to get the best runtimes. 

A few aspects of our experimental methods should be made explicit. Since Thrust function calls are asynchronous, it is difficult to time the actual executions of our operations. Thus our timer was the UNIX time utility. To isolate the function runtimes from the overhead of copying memory from the CPU to the GPU, we timed the execution of that copy without any actual function execution and subtracted that time away from the runs when we issued function calls. The overhead, however, was nontrivial. Since we are using Thrust vectors instead of C-style pointers, the CUDA code spend less time allocating space than the Thrust code. Specifically, in the \code{map} experiments, memory allocation using Thrust took (on average) a full 71.9 \% of the runtime compared to only 30.4 \% using CUDA. So while each operation is comparable, CUDA definitely beats Thrust on memory transfer. We don't view this as a big problem because we expect the average use case of our language to involve allocating the data and then running many consecutive computationally expensive tasks. As computation increases, the significance of memory transfer decreases.

The CUDA and sequential code was a mix of code we wrote and precompiled executables to which we did not have access to the source ({\bf is this true??}). All sequential code was handwritten, and since sequential maps, reductions, and sorts are very straightforward, we believe this is reasonable. The C++ STL \code{sort} function was used, which runs in $O(nlog(n))$ time. We wrote the CUDA \code{map} function, which consists of a single kernel call (as shown in Code Snippet \ref{code:cudaExperiment}). The CUDA \code{reduce} and \code{sort} were precompiled and thus we were limited to the given array sizes. 

\section{Conclusion}
\label{sec:conclusion}

\section{Future Work}
\label{sec:future}

Chestnut is still in its early stages. We have a working solution to many of the problems that make GPGPU programming difficult, but there is still much room for expansion. In this section we present some options for the future of this project.

\subsection{Custom Functions}

A significant limitation with the current Chestnut implementation is that it doesn't support the creation of custom functions from within Chestnut code or from the GUI. This complicates the use of Chestnut for applications which have some inter-dependence of data. One example of such an application is Conway's Game of Life \footnote{\url{http://en.wikipedia.org/wiki/Conway's_Game_of_Life}} which requires knowledge of the 8 neighboring squares to calculate the game state for any given square (TODO: Figure out citing). Custom functions would need significant limits on how they could access memory to ensure that the data-centric GPU model is followed, but we think that they are possible. A custom function would have a limited syntax and would be able to assign the values to each bucket based on the value at that bucket and the values of other nearby or arbitrary buckets. A sample syntax for custom functions could look like the following

\begin{Code Snippet}
\begin{verbatim}
(TODO: Talk to ryan about this!)
function average(Input in, Output out) {
    value = above + below + left + right;
    value = value / 4;
}
\end{verbatim}
\end{Code Snippet}

where the specific keywords \code{above, below, left, right} correspond to the values in the array at those positions relative to the current bucket and the keyword \code{value} represents the value in the output array of the current bucket.

\subsection{Custom DataTypes}

Chestnut currently supports integers and real values (floats), but both of these are primitive datatypes. It would be nice to support more complex datatypes consisting of the primitive types. In the case of sorting this allows for finding a specific entry by sorting on some subfield.

TODO: insert array that shows this?

\subsection{More Predefined Functions/Data Types}

Doubles, pairs, c style data structures?

\subsection{Refine GUI}

As features are added to the Chestnut language it is important to continue to find ways to have them cleanly map into the GUI. Custom functions should appear alongside predefined functions and should follow the same semantics for advertising their available inputs and outputs as sinks and sources. The GUI also currently lacks any way of saving or loading a workspace. It would also be important to streamline the build process. Currently it takes quite a few steps and it should be more of a one click process if the user is unfamiliar with the different parts that make up the Chestnut system.

\subsection{For Loops}

\bibliographystyle{apacite}
\bibliography{references}

\end{document}

